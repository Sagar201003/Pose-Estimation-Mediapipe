{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d9a10b",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Real-Time 3D Pose Detection & Pose Classification with Mediapipe and Python</font> </center>**\n",
    "\n",
    "Pose Detection (also known as Pose Estimation) is a widely used computer vision task that enables you to predict humans poses in images or videos by localizing the key body joints (also reffered  as landmarks), these are elbows, shoulders, and knees, etc. \n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1hT-lhDvzft8vVQv6ObSok73h7A4l5CXf'>\n",
    "\n",
    "\n",
    "[MediaPipe](https://google.github.io/mediapipe/solutions/pose.html) provides a robust solution capable of predicting **thirty-three 3D landmarks** on a human body in real-time with high accuracy even on CPU. It utilizes a two-step machine learning pipeline, by using a detector it first localizes the person within the frame and then uses the pose landmarks detector to predict the  landmarks within the region of interest.\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1c1vFXlRbN9r4ONKIE3sEmaLsWEfm2vpF'>\n",
    "\n",
    "\n",
    "For the videos, the detector is used only for the very first frame and then the ROI is derived from the previous frame’s pose landmarks using a tracking method. Also when the tracker loses track of the identify body pose presence in a frame, the detector is invoked again for the next frame which reduces the computation and latency. The image below shows the thirty-three pose landmarks along with their indexes.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1CDO0KiXZEOuWc7xLEm7EFLLQf2hydCoI\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9a5c7",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "Let's start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91dbd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (0.10.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (23.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (1.25.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.8.0.76)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shiva\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe46c51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc2d54",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initialize the Pose Detection Model</font>**\n",
    "\n",
    "The first thing that we need to do is initialize the pose class using the **`mp.solutions.pose`** syntax and then we will call the setup function **`mp.solutions.pose.Pose()`** with the arguments:\n",
    "\n",
    "* **`static_image_mode`** - It is a boolean value that is if set to `False`, the detector is only invoked as needed, that is in the very first frame or when the tracker loses track. If set to `True`, the person detector is invoked on every input image. So you should probably set this value to True when working with a bunch of unrelated images not videos. Its default value is `False`.\n",
    "\n",
    "* **`min_detection_confidence`** - It is the minimum detection confidence with range `(0.0 , 1.0)` required to consider the person-detection model's prediction correct. Its default value is `0.5`. This means if the detector has a prediction confidence of greater or equal to 50% then it will be considered as a positive detection.\n",
    "\n",
    "\n",
    "* **`min_tracking_confidence`** - It is the minimum tracking confidence `([0.0, 1.0])` required to consider the landmark-tracking model's tracked pose landmarks valid. If the confidence is less than the set value then the detector is invoked again in the next frame/image, so increasing its value increases the robustness, but also increases the latency. Its default value is `0.5`.\n",
    "\n",
    "\n",
    "* **`model_complexity`** - It is the complexity of the pose landmark model. As there are three different models to choose from so the possible values are `0`, `1`, or `2`. The higher the value, the more accurate the results are, but at the expense of higher latency. Its default value is `1`.\n",
    "\n",
    "\n",
    "* **`smooth_landmarks`** - It is a boolean value that is if set to `True`, pose landmarks across different frames are filtered to reduce noise. But only works when **`static_image_mode`** is also set to `False`. Its default value is `True`.\n",
    "\n",
    "Then we will also initialize **`mp.solutions.drawing_utils`** class that will allow us to visualize the landmarks after detection, instead of using this, you can also use OpenCV to visualize the landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e9b06e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initializing mediapipe pose class.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mp_pose \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mpose\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Setting up the Pose function.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pose \u001b[38;5;241m=\u001b[39m mp_pose\u001b[38;5;241m.\u001b[39mPose(static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, model_complexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mp' is not defined"
     ]
    }
   ],
   "source": [
    "# Initializing mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Setting up the Pose function.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Initializing mediapipe drawing class, useful for annotation.\n",
    "mp_drawing = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea6951",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Read an Image</font>**\n",
    "\n",
    "Now we will read a sample image using the function [**`cv2.imread()`**](https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56) and then display that image using the [**`matplotlib`**](https://matplotlib.org/stable/index.html) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image from the specified path.\n",
    "sample_img = cv2.imread('media/sample.jpg')\n",
    "\n",
    "# Specify a size of the figure.\n",
    "plt.figure(figsize = [10, 10])\n",
    "\n",
    "# Display the sample image, also convert BGR to RGB for display. \n",
    "plt.title(\"Sample Image\");plt.axis('off');plt.imshow(sample_img[:,:,::-1]);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f267f",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Perform Pose Detection</font>**\n",
    "\n",
    "Now we will pass the image to the pose detection machine learning pipeline by using the function **`mp.solutions.pose.Pose().process()`**. But the pipeline expects the input images in **`RGB`** color format so first we will have to convert the sample image from **`BGR`** to **`RGB`** format using the function [**`cv2.cvtColor()`**](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab) as OpenCV reads images in **`BGR`** format (instead of **`RGB`**).\n",
    "\n",
    "After performing the pose detection, we will get a list of thirty-three landmarks representing the body joint locations of the prominent person in the image. Each landmark has:\n",
    "\n",
    "* **`x`** - It is the landmark x-coordinate normalized to [0.0, 1.0] by the image width.\n",
    "\n",
    "\n",
    "* **`y`**: It is the landmark y-coordinate normalized to [0.0, 1.0] by the image height.\n",
    "\n",
    "\n",
    "* **`z`**: It is the landmark z-coordinate normalized to roughly the same scale as **`x`**. It represents the landmark depth with midpoint of hips being the origin, so the smaller the value of z, the closer the landmark is to the camera. \n",
    "\n",
    "* **`visibility`**: It is a value with range [0.0, 1.0] representing the possibility of the landmark being visible (not occluded) in the image. This is a useful variable when deciding if you want to show a particular joint because it might be occluded or partially visible in the image.\n",
    "\n",
    "After performing the pose detection on the sample image above, we will display the first two landmarks from the list, so that you get a better idea of the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa219308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pose detection after converting the image into RGB format.\n",
    "results = pose.process(cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Check if any landmarks are found.\n",
    "if results.pose_landmarks:\n",
    "    \n",
    "    # Iterate two times as we only want to display first two landmarks.\n",
    "    for i in range(2):\n",
    "        \n",
    "        # Display the found normalized landmarks.\n",
    "        print(f'{mp_pose.PoseLandmark(i).name}:\\n{results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1b912",
   "metadata": {},
   "source": [
    "Now we will convert the two normalized landmarks displayed above into their original scale by using the width and height of the  image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c88ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the height and width of the sample image.\n",
    "image_height, image_width, _ = sample_img.shape\n",
    "\n",
    "# Check if any landmarks are found.\n",
    "if results.pose_landmarks:\n",
    "    \n",
    "    # Iterate two times as we only want to display first two landmark.\n",
    "    for i in range(2):\n",
    "        \n",
    "        # Display the found landmarks after converting them into their original scale.\n",
    "        print(f'{mp_pose.PoseLandmark(i).name}:') \n",
    "        print(f'x: {results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].x * image_width}')\n",
    "        print(f'y: {results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].y * image_height}')\n",
    "        print(f'z: {results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].z * image_width}')\n",
    "        print(f'visibility: {results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].visibility}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4949ad",
   "metadata": {},
   "source": [
    "Now we will draw the detected landmarks on the sample image using the function **`mp.solutions.drawing_utils.draw_landmarks()`** and display the resultant image using the [**`matplotlib`**](https://matplotlib.org/stable/index.html) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the sample image to draw landmarks on.\n",
    "img_copy = sample_img.copy()\n",
    "\n",
    "# Check if any landmarks are found.\n",
    "if results.pose_landmarks:\n",
    "    \n",
    "    # Draw Pose landmarks on the sample image.\n",
    "    mp_drawing.draw_landmarks(image=img_copy, landmark_list=results.pose_landmarks, connections=mp_pose.POSE_CONNECTIONS)\n",
    "       \n",
    "    # Specify a size of the figure.\n",
    "    fig = plt.figure(figsize = [10, 10])\n",
    "\n",
    "    # Display the output image with the landmarks drawn, also convert BGR to RGB for display. \n",
    "    plt.title(\"Output\");plt.axis('off');plt.imshow(img_copy[:,:,::-1]);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b6b78",
   "metadata": {},
   "source": [
    "Now we will go a step further and visualize the landmarks in three-dimensions (3D) using the function **`mp.solutions.drawing_utils.plot_landmarks()`**. We will need the POSE_WORLD_LANDMARKS that is another list of pose landmarks in world coordinates that has the 3D coordinates in meters with the origin at the center between the hips of the person. \n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1JPQCLOPZkS8b3VBNtmc2aOfuAjhddSMZ' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pose landmarks in 3D.\n",
    "mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f90f9",
   "metadata": {},
   "source": [
    "**Note:** This is actually a neat hack by mediapipe, the coordinates returned are not actually in **3D** but by setting hip landmark as the origin allows us to measure relative distance of the other points from the hip, and since this distance increases or decreases depending upon if you're close or further from the camera it gives us a sense of depth of each landmark point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325fad9",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Pose Detection Function</font>**\n",
    "\n",
    "Now we will put all this together to create a function that will perform pose detection on an image and will visualize the results or return the results depending upon the passed arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image, pose, display=True):\n",
    "    '''\n",
    "    This function performs pose detection on an image.\n",
    "    Args:\n",
    "        image: The input image with a prominent person whose pose landmarks needs to be detected.\n",
    "        pose: The pose setup function required to perform the pose detection.\n",
    "        display: A boolean value that is if set to true the function displays the original input image, the resultant image, \n",
    "                 and the pose landmarks in 3D plot and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The input image with the detected pose landmarks drawn.\n",
    "        landmarks: A list of detected landmarks converted into their original scale.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the input image.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Convert the image from BGR into RGB format.\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform the Pose Detection.\n",
    "    results = pose.process(imageRGB)\n",
    "    \n",
    "    # Retrieve the height and width of the input image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the detected landmarks.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Check if any landmarks are detected.\n",
    "    if results.pose_landmarks:\n",
    "    \n",
    "        # Draw Pose landmarks on the output image.\n",
    "        mp_drawing.draw_landmarks(image=output_image, landmark_list=results.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Iterate over the detected landmarks.\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            \n",
    "            # Append the landmark into the list.\n",
    "            landmarks.append((int(landmark.x * width), int(landmark.y * height),\n",
    "                                  (landmark.z * width)))\n",
    "    \n",
    "    # Check if the original input image and the resultant image are specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the original input image and the resultant image.\n",
    "        plt.figure(figsize=[22,22])\n",
    "        plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "        # Also Plot the Pose landmarks in 3D.\n",
    "        mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the found landmarks.\n",
    "        return output_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707df2f4",
   "metadata": {},
   "source": [
    "Now we will utilize the function created above to perform pose detection on a few sample images and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose detection on it.\n",
    "image = cv2.imread('media/sample1.jpg')\n",
    "detectPose(image, pose, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose detection on it.\n",
    "image = cv2.imread('media/sample2.jpg')\n",
    "detectPose(image, pose, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose detection on it.\n",
    "image = cv2.imread('media/sample3.jpg')\n",
    "detectPose(image, pose, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f30de2",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Pose Detection On Real-Time Webcam Feed/Video</font>**\n",
    "\n",
    "The results on the images were pretty good, now we will try the function on a real-time webcam feed and a video. Depending upon whether you want to run pose detection on a video stored in the disk or on the webcam feed, you can comment and uncomment the initialization code of the VideoCapture object accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c77c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Pose function for video.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "video = cv2.VideoCapture(1)\n",
    "\n",
    "# Create named window for resizing purposes\n",
    "cv2.namedWindow('Pose Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "# Initialize the VideoCapture object to read from a video stored in the disk.\n",
    "#video = cv2.VideoCapture('media/running.mp4')\n",
    "\n",
    "# Set video camera size\n",
    "video.set(3,1280)\n",
    "video.set(4,960)\n",
    "\n",
    "# Initialize a variable to store the time of the previous frame.\n",
    "time1 = 0\n",
    "\n",
    "# Iterate until the video is accessed successfully.\n",
    "while video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = video.read()\n",
    "    \n",
    "    # Check if frame is not read properly.\n",
    "    if not ok:\n",
    "        \n",
    "        # Break the loop.\n",
    "        break\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the width and height of the frame\n",
    "    frame_height, frame_width, _ =  frame.shape\n",
    "    \n",
    "    # Resize the frame while keeping the aspect ratio.\n",
    "    frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "    \n",
    "    # Perform Pose landmark detection.\n",
    "    frame, _ = detectPose(frame, pose_video, display=False)\n",
    "    \n",
    "    # Set the time for this frame to the current time.\n",
    "    time2 = time()\n",
    "    \n",
    "    # Check if the difference between the previous and this frame time > 0 to avoid division by zero.\n",
    "    if (time2 - time1) > 0:\n",
    "    \n",
    "        # Calculate the number of frames per second.\n",
    "        frames_per_second = 1.0 / (time2 - time1)\n",
    "        \n",
    "        # Write the calculated number of frames per second on the frame. \n",
    "        cv2.putText(frame, 'FPS: {}'.format(int(frames_per_second)), (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "    \n",
    "    # Update the previous frame time to this frame time.\n",
    "    # As this frame will become previous frame in next iteration.\n",
    "    time1 = time2\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    \n",
    "    # Wait until a key is pressed.\n",
    "    # Retreive the ASCII code of the key pressed\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed.\n",
    "    if(k == 27):\n",
    "        \n",
    "        # Break the loop.\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object.\n",
    "video.release()\n",
    "\n",
    "# Close the windows.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f19f8",
   "metadata": {},
   "source": [
    "Cool! so it works great on the videos too. The model is pretty fast and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0ca1e",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Pose Classification with Angle Heuristics</font>**\n",
    "\n",
    "We have learned to perform pose detection, now we will level up our game by also classifying different yoga poses using the calculated angles of various joints. We will first detect the pose landmarks and then use them to compute angles between joints and depending upon those angles we will recognize the yoga pose of the prominent person in an image.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1lhAcgq2jy5NavGQeYjTwwEAZIu6Aeypg' width=500>\n",
    "\n",
    "But this approach does have a drawback that limits its use to a controlled environment, the calculated angles vary with the angle between the person and the camera. So the person needs to be facing the camera straight to get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a93a1",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Function to Calculate Angle between Landmarks</font>**\n",
    "\n",
    "Now we will create a function that will be capable of calculating angles between three landmarks. The angle between landmarks? Do not get confused, as this is the same as calculating the angle between two lines. \n",
    "\n",
    "The first point (landmark) is considered as the starting point of the first line, the second point (landmark) is considered as the ending point of the first line and the starting point of the second line as well, and the third point (landmark) is considered as the ending point of the second line.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1KSN2OnenNMZ7Jwai_E1jeWdP5Mzay3Ad' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c85cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAngle(landmark1, landmark2, landmark3):\n",
    "    '''\n",
    "    This function calculates angle between three different landmarks.\n",
    "    Args:\n",
    "        landmark1: The first landmark containing the x,y and z coordinates.\n",
    "        landmark2: The second landmark containing the x,y and z coordinates.\n",
    "        landmark3: The third landmark containing the x,y and z coordinates.\n",
    "    Returns:\n",
    "        angle: The calculated angle between the three landmarks.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Get the required landmarks coordinates.\n",
    "    x1, y1, _ = landmark1\n",
    "    x2, y2, _ = landmark2\n",
    "    x3, y3, _ = landmark3\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    \n",
    "    # Check if the angle is less than zero.\n",
    "    if angle < 0:\n",
    "\n",
    "        # Add 360 to the found angle.\n",
    "        angle += 360\n",
    "    \n",
    "    # Return the calculated angle.\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d1f2a",
   "metadata": {},
   "source": [
    "Now we will test the function created above to calculate angle three landmarks with dummy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the angle between the three landmarks.\n",
    "angle = calculateAngle((558, 326, 0), (642, 333, 0), (718, 321, 0))\n",
    "\n",
    "# Display the calculated angle.\n",
    "print(f'The calculated angle is {angle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9215701",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Function to Perform Pose Classification</font>**\n",
    "\n",
    "Now we will create a function that will be capable of classifying different yoga poses using the calculated angles of various joints. The function will be capable of identifying the following yoga poses:\n",
    "\n",
    "* **Warrior II Pose**\n",
    "* **T Pose**\n",
    "* **Tree Pose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyPose(landmarks, output_image, display=False):\n",
    "    '''\n",
    "    This function classifies yoga poses depending upon the angles of various body joints.\n",
    "    Args:\n",
    "        landmarks: A list of detected landmarks of the person whose pose needs to be classified.\n",
    "        output_image: A image of the person with the detected pose landmarks drawn.\n",
    "        display: A boolean value that is if set to true the function displays the resultant image with the pose label \n",
    "        written on it and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image with the detected pose landmarks drawn and pose label written.\n",
    "        label: The classified pose label of the person in the output_image.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Initialize the label of the pose. It is not known at this stage.\n",
    "    label = 'Unknown Pose'\n",
    "\n",
    "    # Specify the color (Red) with which the label will be written on the image.\n",
    "    color = (0, 0, 255)\n",
    "    \n",
    "    # Calculate the required angles.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Get the angle between the left shoulder, elbow and wrist points. \n",
    "    left_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value])\n",
    "    \n",
    "    # Get the angle between the right shoulder, elbow and wrist points. \n",
    "    right_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value])   \n",
    "    \n",
    "    # Get the angle between the left elbow, shoulder and hip points. \n",
    "    left_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_HIP.value])\n",
    "\n",
    "    # Get the angle between the right hip, shoulder and elbow points. \n",
    "    right_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value])\n",
    "\n",
    "    # Get the angle between the left hip, knee and ankle points. \n",
    "    left_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_HIP.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value])\n",
    "\n",
    "    # Get the angle between the right hip, knee and ankle points \n",
    "    right_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value])\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the warrior II pose or the T pose.\n",
    "    # As for both of them, both arms should be straight and shoulders should be at the specific angle.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the both arms are straight.\n",
    "    if left_elbow_angle > 165 and left_elbow_angle < 195 and right_elbow_angle > 165 and right_elbow_angle < 195:\n",
    "\n",
    "        # Check if shoulders are at the required angle.\n",
    "        if left_shoulder_angle > 80 and left_shoulder_angle < 110 and right_shoulder_angle > 80 and right_shoulder_angle < 110:\n",
    "\n",
    "    # Check if it is the warrior II pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Check if one leg is straight.\n",
    "            if left_knee_angle > 165 and left_knee_angle < 195 or right_knee_angle > 165 and right_knee_angle < 195:\n",
    "\n",
    "                # Check if the other leg is bended at the required angle.\n",
    "                if left_knee_angle > 90 and left_knee_angle < 120 or right_knee_angle > 90 and right_knee_angle < 120:\n",
    "\n",
    "                    # Specify the label of the pose that is Warrior II pose.\n",
    "                    label = 'Warrior II Pose' \n",
    "                        \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the T pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "            # Check if both legs are straight\n",
    "            if left_knee_angle > 160 and left_knee_angle < 195 and right_knee_angle > 160 and right_knee_angle < 195:\n",
    "\n",
    "                # Specify the label of the pose that is tree pose.\n",
    "                label = 'T Pose'\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the tree pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if one leg is straight\n",
    "    if left_knee_angle > 165 and left_knee_angle < 195 or right_knee_angle > 165 and right_knee_angle < 195:\n",
    "\n",
    "        # Check if the other leg is bended at the required angle.\n",
    "        if left_knee_angle > 315 and left_knee_angle < 335 or right_knee_angle > 25 and right_knee_angle < 45:\n",
    "\n",
    "            # Specify the label of the pose that is tree pose.\n",
    "            label = 'Tree Pose'\n",
    "                \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the pose is classified successfully\n",
    "    if label != 'Unknown Pose':\n",
    "        \n",
    "        # Update the color (to green) with which the label will be written on the image.\n",
    "        color = (0, 255, 0)  \n",
    "    \n",
    "    # Write the label on the output image. \n",
    "    cv2.putText(output_image, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "    \n",
    "    # Check if the resultant image is specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the resultant image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the classified label.\n",
    "        return output_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16762c47",
   "metadata": {},
   "source": [
    "Now we will utilize the function created above to perform pose classification on a few images of people and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583aa7a3",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Warrior II Pose</font>**\n",
    "\n",
    "The Warrior II Pose (also known as Virabhadrasana II) is the same pose that the person is making in the image above. It can be classified using the following combination of body part angles:\n",
    "\n",
    "* Around 180° at both elbows\n",
    "* Around 90° angle at both shoulders\n",
    "* Around 180° angle at one knee\n",
    "* Around 90° angle at the other knee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/warriorIIpose.jpg')\n",
    "output_image, landmarks = detectPose(image, pose, display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/warriorIIpose1.jpg')\n",
    "output_image, landmarks = detectPose(image, pose, display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489eacab",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Tree Pose</font>**\n",
    "\n",
    "Tree Pose (also known as Vrikshasana) is another yoga pose for which the person has to keep one leg straight and bend the other leg at a required angle. The pose can be classified easily using the following combination of body part angles:\n",
    "\n",
    "* Around 180° angle at one knee\n",
    "* Around 35° (if right knee) or 335° (if left knee) angle at the other knee\n",
    "\n",
    "Now to understand it better, you should go back to the pose classification function above to overview the classification code of this yoga pose.\n",
    "\n",
    "We will perform pose classification on a few images of people in the tree yoga pose and display the results using the same function we had created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/treepose.jpg')\n",
    "output_image, landmarks = detectPose(image, mp_pose.Pose(static_image_mode=True,\n",
    "                                         min_detection_confidence=0.5, model_complexity=0), display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/treepose1.jpg')\n",
    "output_image, landmarks = detectPose(image, mp_pose.Pose(static_image_mode=True,\n",
    "                                         min_detection_confidence=0.5, model_complexity=0), display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/treepose2.jpg')\n",
    "output_image, landmarks = detectPose(image, pose, display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e572da",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">T Pose</font>**\n",
    "\n",
    "T Pose (also known as a bind pose or reference pose) is the last pose we are dealing with in this lesson. To make this pose, one has to stand up like a tree with both hands wide open as branches. The following body part angles are required to make this one:\n",
    "\n",
    "* Around 180° at both elbows\n",
    "* Around 90° angle at both shoulders\n",
    "* Around 180° angle at both knees\n",
    "\n",
    "You can now go back to go through the classification code of this T pose in the pose classification function created above.\n",
    "\n",
    "Now, let's test the pose classification function on a few images of the T pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/Tpose.jpg')\n",
    "output_image, landmarks = detectPose(image, pose, display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cafa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/Tpose1.jpg')\n",
    "output_image, landmarks = detectPose(image, pose, display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7f9a7",
   "metadata": {},
   "source": [
    "So the function is working pretty well on all the known poses on images lets try it on an unknown pose called cobra pose (also known as Bhujangasana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947270c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another sample image and perform pose classification on it.\n",
    "image = cv2.imread('media/cobrapose1.jpg')\n",
    "output_image, landmarks = detectPose(image, pose, display=False)\n",
    "if landmarks:\n",
    "    classifyPose(landmarks, output_image, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226fcd0",
   "metadata": {},
   "source": [
    "Now if you want you can extend the pose classification function to make it capable of identifying more yoga poses like the one in the image above. The following combination of body part angles can help classify this one:\n",
    "\n",
    "* Around 180° angle at both knees\n",
    "* Around 105° (if the person is facing right side) or 240° (if the person is facing left side) angle at both hips\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Pose Classification On Real-Time Webcam Feed</font>**\n",
    "\n",
    "Now we will test the function created above to perform the pose classification on a real-time webcam feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Pose function for video.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Initialize a resizable window.\n",
    "cv2.namedWindow('Pose Classification', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly.\n",
    "    if not ok:\n",
    "        \n",
    "        # Continue to the next iteration to read the next frame and ignore the empty camera frame.\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the width and height of the frame\n",
    "    frame_height, frame_width, _ =  frame.shape\n",
    "    \n",
    "    # Resize the frame while keeping the aspect ratio.\n",
    "    frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "    \n",
    "    # Perform Pose landmark detection.\n",
    "    frame, landmarks = detectPose(frame, pose_video, display=False)\n",
    "    \n",
    "    # Check if the landmarks are detected.\n",
    "    if landmarks:\n",
    "        \n",
    "        # Perform the Pose Classification.\n",
    "        frame, _ = classifyPose(landmarks, frame, display=False)\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Pose Classification', frame)\n",
    "    \n",
    "    # Wait until a key is pressed.\n",
    "    # Retreive the ASCII code of the key pressed\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed.\n",
    "    if(k == 27):\n",
    "        \n",
    "        # Break the loop.\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e92151",
   "metadata": {},
   "source": [
    "As expected, the results were amazing, if you were having difficulty in making the poses you can expand the range of angles used in the classification function, but that may open up the possibility of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0da731",
   "metadata": {},
   "source": [
    "#  <center> <font style=\"color:rgb(234,19,148)\">Join My Mediapipe Course</font>   </center>\n",
    "\n",
    "You can now join the waitlist for my brand new upcoming course on Mediapipe, I’m not going to any details now but I’m just going to say this course will be a Blast, to say the least. This will be a completely application-oriented course and it will train you on how to create State of the Art exciting applications.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1CP0jp5rlTkOuj23PzUeGGi3NknXVI3wi'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>  <a href=\"https://www.getdrip.com/forms/677961673/submissions/new\"> <button>Join Now!</button>\n",
    "</a></center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef042cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
